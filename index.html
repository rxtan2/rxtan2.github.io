<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Reuben Tan</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Reuben Tan</name>
              </p>
              <p>I am currently a researcher in the <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/">Deep Learning Group</a> at Microsoft Research, Redmond working under <a href="http://research.microsoft.com/en-us/um/people/jfgao/">Dr. Jianfeng Gao</a>.  During my PhD, I was very fortunate to be advised by Professors <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a> and <a href="https://bryanplummer.com/">Bryan Plummer</a>. I am also very grateful to be able to collaborate closely with Dr <a href="https://bryanrussell.org/">Bryan Russell</a> at Adobe Research. My primary research interests fall under the general umbrella of multimodal learning and video understanding. If you are interested in collaborating or having any discussions on multimodal foundation models, please feel free to reach out!  
              </p>
              <!--<p style="color:#FF0000";><b> I am currently looking for full-time research positions starting from Summer 2024.</b> </p>-->
              <p style="text-align:center">
                <a href="mailto:tanreuben@microsoft.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/rxtan2/">Github</a> &nbsp/&nbsp
				<a href="https://scholar.google.com/citations?user=rPRWQHgAAAAJ&hl=en&authuser=1&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="">CV</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research interests</heading>
              <p>Computer Vision, Video Understanding, Multimodal learning, Multimodal-LLMs </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/magma-teaser.png" alt="magma" style="width: 300px; height: auto;">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Magma: A Foundation Model for Multimodal AI Agents</papertitle>
    <br>
    <strong>Magma Team</strong>
    <br>
    <em>CVPR</em>, 2025
    <br>
    <a href="https://microsoft.github.io/Magma/">Project page</a>
    /
    <a href="https://github.com/microsoft/Magma">Code</a>
    /
    <a href="https://www.arxiv.org/pdf/2502.13130">arXiv</a>
    <p>Magma is the first foundation model for multimodal AI agents. As the bedrock for mutimodal agentic models, it possesse strong capabilities to perceive the multimodal groundingly world AND take goal-driven actions precisely. By effectively transferring knowledge from freely available visual and language data, Magma bridges verbal, spatial and temporal intelligence to navigate complex tasks and settings across digital and physical world.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/lapa-teaser.png" alt="avset"  style="width: 300px; height: auto;">
  </td>
  <td width="75%" valign="middle">
    <papertitle>LAPA: Latent Action Pretraining from Videos</papertitle>
    <br>
    <a href="https://seonghyeonye.github.io/">Seonghyeon Ye</a>,
    <a href="https://joeljang.github.io/">Joel Jang</a>,
    <a href="https://scholar.google.com/citations?user=_Kw32VoAAAAJ&hl=ko">Byeongguk Jeon</a>,
    <a href="https://scholar.google.co.kr/citations?user=xii168wAAAAJ&hl">Sejune Joo</a>,
    <a href="https://jwyang.github.io/">Jianwei Yang</a>,
    <a href="https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl">Baolin Peng</a>,
    <a href="https://ai.stanford.edu/~amandlek/">Ajay Mandlekar</a>,
    <strong>Reuben Tan</strong>,
    <a href="https://research.nvidia.com/person/yu-wei-chao">Yu-Wei Chao</a>,
    
    <a href="https://yuchenlin.xyz/">Yuchen Lin</a>,
    <a href="https://scholar.google.com/citations?user=r4KX3UgAAAAJ&hl">Lars Liden</a>,
    <a href="https://sites.google.com/view/kiminlee">Kimin Lee</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jfgao/?from=https://research.microsoft.com/en-us/um/people/jfgao/&type=exact">Jianfeng Gao</a>,
    <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a>,
    <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>
    <a href="https://seominjoon.github.io/">Minjoon Seo</a>
    <br>
    <em>ICLR</em>, 2025
    <br>
    <a href="https://latentactionpretraining.github.io/">Project page</a>
    /
    <a href="https://github.com/LatentActionPretraining/LAPA">Code</a>
    /
    <a href="https://arxiv.org/abs/2410.11758">arXiv</a>
    <p>We introduce Latent Action Pretraining for general Action models (LAPA), the first unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/temporalbench-teaser.png" alt="temporalbench"  style="width: 300px; height: auto;">
  </td>
  <td width="75%" valign="middle">
    <papertitle>TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models</papertitle>
    <br>
    <a href="https://pages.cs.wisc.edu/~mucai/">Mu Cai</a>,
    <strong>Reuben Tan</strong>,
    <a href="https://pages.cs.wisc.edu/~harrisz/">Jianrui Zhang</a>,
    <a href="https://www.bocheng.me/">Bocheng Zou</a>,

    <a href="https://drogozhang.github.io/">Kai Zhang</a>,
    <a href="https://yaof20.github.io/">Feng Yao</a>,
    <a href="https://fangruizhu.github.io/">Fangrui Zhu</a>,
    <a href="https://g-jing.github.io/">Jing Gu</a>,
    <a href="https://scholar.google.com/citations?user=irrbH_IAAAAJ&hl=en">Yiwu Zhong</a>,
    
    <a href="https://42shawn.github.io/">Yuzhang Shang</a>,
    
    <a href="https://yao-dou.github.io/">Yao Dou</a>,
    <a href="https://jadenpark0.github.io/">Jaden Park</a>,
    <a href="https://www.microsoft.com/en-us/research/people/jfgao/?from=https://research.microsoft.com/en-us/um/people/jfgao/&type=exact">Jianfeng Gao</a>,
    <a href="https://pages.cs.wisc.edu/~yongjaelee/">Yong Jae Lee</a>,
    <a href="https://pages.cs.wisc.edu/~yongjaelee/">Jianwei Yang</a>
    <br>
    <em>arXiv</em>, 2025
    <br>
    <a href="https://temporalbench.github.io/">Project page</a>
    /
    <a href="https://github.com/mu-cai/TemporalBench  ">Code</a>
    /
    <a href="https://arxiv.org/abs/2410.10818">arXiv</a>
    <p>We introduce the TemporalBench evaluation dataset for evaluating the capability of multimodal video models to understand fine-grained temporal dynamics in videos.</p>
  </td>
</tr>
        
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/koala-motiv-fig.png" alt="avset" width="300" height="300">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Koala: Key frame-conditioned long video-LLM</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://cs-people.bu.edu/sunxm/">Ximeng Sun</a>,
    <a href="https://feinanshan.github.io/">Ping Hu</a>,
    <a href="http://juiwang.com/">Jui-hsien Wang</a>,
    <a href="https://scholar.google.com/citations?user=3EIDlFwAAAAJ&hl=en">Hanieh Deilamsalehy</a>,
    <a href="https://https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="https://bryanrussell.org/">Bryan Russell</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
    <br>
    <em>CVPR</em>, 2024
    <br>
    <a href="https://cs-people.bu.edu/rxtan/projects/Koala/">Project page</a>
    /
    <a href="https://github.com/rxtan2/Koala-video-llm">Code</a>
    /
    <a href="https://arxiv.org/abs/2404.04346">arXiv</a>
    <p>We propose a lightweight and self-supervised approach that introduces learnable spatiotemporal queries to adapt pretrained video-LLMs for generalizing to much longer videos. Our approach introduces a global-local video Qformer with two new modules that use the global video context to compute contextual tokens for understanding short and long video moments. We train our proposed approach on HowTo100M and demonstrate its effectiveness on zero-shot long video understanding benchmarks.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/socrastis_teaser_fig.png" alt="avset" width="300" height="200">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Socratis: Are large multimodal models emotionally aware?</papertitle>
    <br>
    <a href="">Katherine Deng</a>,
    <a href="https://arijitray1993.github.io/">Arijit Ray</a>,
    <strong>Reuben Tan</strong>,
    <a href="https://saadia-gabriel.github.io/">Saadia Gabriel</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>  
    <br>
    <em>ICCV Wecia</em>, 2023
    <br>
    <a href="">Project page</a>
    /
    <a href="https://github.com/arijitray1993/socratis">Dataset</a>
    /
    <a href="https://arxiv.org/pdf/2308.16741.pdf">arXiv</a>
    <p>We propose Socratis, a societal reactions bench-mark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/mvp_motivational_figure.png" alt="avset" width="300" height="250">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Multiscale Video Pretraining for Long-Term Activity Forecasting</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://mattdl.github.io/">Matthias De Lange</a>,
    <a href="https://scholar.google.com/citations?user=cjmjU5AAAAAJ&hl=en">Michael Iuzzolino</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="https://scholar.google.com/citations?user=WiWKsngAAAAJ&hl=en">Karl Ridgeway</a>,
    <a href="https://ltorresa.github.io/home.html">Lorenzo Torresani</a>  
    <br>
    <em>Submission</em>, 2023
    <br>
    <a href="">Project page</a>
    /
    <a href="">Code</a>
    /
    <a href="">arXiv</a>
    <p>Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel self-supervised pretraining approach that learns robust representations for forecasting by learning to predict contextualized representations of future video clips over multiple timescales. MVP is based on our observation that actions in videos have a multiscale nature, where atomic actions typically occur at a short timescale and more complex actions may span longer timescales. We compare MVP to state-of-the-art self-supervised video learning approaches on downstream long-term forecasting tasks including long-term action anticipation and video summary prediction.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/av_sep_motivational_fig.png" alt="avset" width="300" height="300">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Language-Guided Audio-Visual Source Separation via Trimodal Consistency</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://arijitray1993.github.io/">Arijit Ray</a>,
    <a href="https://cs-people.bu.edu/aburns4/">Andrea Burns</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="https://www.justinsalamon.com/">Justin Salamon</a>,
    <a href="https://www.urinieto.com/about/">Oriol Nieto</a>,
    <a href="https://bryanrussell.org/">Bryan Russell</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
    <br>
    <em>CVPR</em>, 2023
    <br>
    <a href="https://cs-people.bu.edu/rxtan/projects/VAST">Project page</a>
    /
    <a href="https://github.com/rxtan2/AVSeT">Code</a>
    /
    <a href="https://arxiv.org/abs/2303.16342">arXiv</a>
    <p>We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities.  During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/eccv2022_overview_img.png" alt="avset" width="300" height="150">
  </td>
  <td width="75%" valign="middle">
    <papertitle>NewsStories: Illustrating articles with visual summaries</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="http://www.scribblethink.org/">J.P Lewis</a>,
    <a href="https://scholar.google.com/citations?user=zc6Iy0IAAAAJ&hl=en">Avneesh Sud</a>,
    <a href="https://scholar.google.com/citations?user=sUK_w2QAAAAJ&hl=en">Thomas Leung.</a>
    <br>
    <em>ECCV</em>, 2022
    <br>
    <a href="https://newsstoriesdata.github.io/newsstories.github.io/">Project page</a>
    /
    <a href="https://github.com/NewsStoriesData/newsstories.github.io">Code</a>
    /
    <a href="https://arxiv.org/abs/2207.13061">arXiv</a>
    <p>Recent self-supervised approaches have used large-scale image-text datasets to learn powerful representations that transfer to many tasks without finetuning. These methods often assume that there is a one-to-one correspondence between images and their (short) captions. However, many tasks require reasoning about multiple images paired
with a long text narrative, such as photos in a news article. In this work, we explore a novel setting where the goal is to learn a self-supervised visual-language representation from longer text paired with a set of photos, which we call visual summaries. In addition, unlike prior work which assumed captions have a literal relation to the image, we assume images only contain loose illustrative correspondence with the text. To explore this problem, we introduce a large-scale multimodal dataset called NewsStories containing over 31M articles, 22M images and 1M videos.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/grounding_narrations.jpg" alt="avset" width="300" height="150">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Look at What I‚Äôm Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="https://sites.google.com/view/hailinjin">Hailin Jin</a>,
    <a href="https://bryanrussell.org/">Bryan Russell</a>
    <br>
    <em>NeurIPS</em>, 2021 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
    <br>
    <a href="https://cs-people.bu.edu/rxtan/projects/grounding_narrations/">Project page</a>
    /
    <a href="https://github.com/rxtan2/video-grounding-narrations/">Code</a>
    /
    <a href="https://arxiv.org/abs/2110.10596">arXiv</a>
    <p>We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations. To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities' representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/tanLoGAN2019.jpg" alt="avset" width="300" height="150">
  </td>
  <td width="75%" valign="middle">
    <papertitle>LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://visionlanguagelab.github.io/">Huijuan Xu</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>
    <br>
    <em>WACV</em>, 2021
    <br>
    <a href="">Project page</a>
    /
    <a href="">Code</a>
    /
    <a href="https://arxiv.org/abs/1909.13784">arXiv</a>
    <p>The goal of weakly-supervised video moment retrieval is to localize the video segment most relevant to the given natural language query without access to temporal annotations during training. Prior strongly- and weakly-supervised approaches often leverage co-attention mechanisms to learn visual-semantic representations for localization. However, while such approaches tend to focus on identifying relationships between elements of the video and language modalities, there is less emphasis on modeling relational context between video frames given the semantic context of the query. Consequently, the above-mentioned visual-semantic representations, built upon local frame features, do not contain much contextual information. To address this limitation, we propose a Latent Graph Co-Attention Network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/Neural_News_Fig_1.jpg" alt="avset" width="300" height="200">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>
    <br>
    <em>EMNLP</em>, 2020
    <br>
    <a href="https://cs-people.bu.edu/rxtan/projects/didan/">Project page</a>
    /
    <a href="https://github.com/rxtan2/DIDAN/">Code</a>
    /
    <a href="https://arxiv.org/abs/2009.07698">arXiv</a>
    <p>Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/learning_sim_conditions.jpg" alt="avset" width="300" height="150">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Learning Similarity Conditions Without Explicit Supervision</papertitle>
    <br>
    <strong>Reuben Tan</strong>,
    <a href="https://scholar.google.com/citations?user=I7LbGLUAAAAJ&hl=en">Mariya I. Vasileva</a>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>
    <br>
    <em>ICCV</em>, 2019
    <br>
    <a href="https://github.com/rxtan2/Learning-Similarity-Conditions">Project page</a>
    /
    <a href="https://github.com/rxtan2/Learning-Similarity-Conditions">Code</a>
    /
    <a href="https://arxiv.org/abs/1908.08589">arXiv</a>
    <p>Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/language2019.jpg" alt="avset" width="300" height="150">
  </td>
  <td width="75%" valign="middle">
    <papertitle>Language Features Matter: Effective Language Representations for Vision-Language Tasks</papertitle>
    <br>
    <a href="https://cs-people.bu.edu/aburns4/">Andrea Burns</a>,
    <strong>Reuben Tan</strong>,
    <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
    <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>,
    <a href="https://bryanplummer.com/">Bryan A. Plummer</a>
    <br>
    <em>ICCV</em>, 2019
    <br>
    <a href="http://ai.bu.edu/grovle/">Project page</a>
    /
    <a href="http://ai.bu.edu/grovle/">Code</a>
    /
    <a href="https://arxiv.org/abs/1908.06327">arXiv</a>
    <p>Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We believe that language features deserve more attention, and conduct experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms an LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we propose a set of best practices for incorporating the language component of VL tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome.</p>
  </td>
</tr>

        </tbody></table>
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <heading>Acknowledgements</heading>
              <p style="text-align:left;font-size:small;">
                This webpage template is copied from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
